{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AfJm0Ad0dApj"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e232e16c1c843e5a3059de970e43b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c34ba1b2ace54577b319f95a820fca5a",
              "IPY_MODEL_615b2cd3ab05426b9dfae2d74878a36e",
              "IPY_MODEL_7c5a0ce99efd49548be601ba64b02636"
            ],
            "layout": "IPY_MODEL_bfdf007f33074cd080ac36bb2a41d53c"
          }
        },
        "c34ba1b2ace54577b319f95a820fca5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d4ef4a2abac4228b7d26cfd60a5925c",
            "placeholder": "​",
            "style": "IPY_MODEL_fa6357cd29644247b46ad03fe24c0679",
            "value": "Epoch 1/1 - Loss: 0.1945697124561538:  18%"
          }
        },
        "615b2cd3ab05426b9dfae2d74878a36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3593345bd79641559bf2e00e954409a4",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ace97cf3152497aafd37e7819b73a99",
            "value": 115
          }
        },
        "7c5a0ce99efd49548be601ba64b02636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04c2d3846a804fd7b1f2496a2db10f7c",
            "placeholder": "​",
            "style": "IPY_MODEL_4e952cc773574dc88e561ce8337a63f6",
            "value": " 115/625 [02:04&lt;09:14,  1.09s/it]"
          }
        },
        "bfdf007f33074cd080ac36bb2a41d53c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d4ef4a2abac4228b7d26cfd60a5925c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6357cd29644247b46ad03fe24c0679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3593345bd79641559bf2e00e954409a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ace97cf3152497aafd37e7819b73a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04c2d3846a804fd7b1f2496a2db10f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e952cc773574dc88e561ce8337a63f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.config.verbose = False\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets"
      ],
      "metadata": {
        "id": "OxEXzDTsTX18"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using {device} device')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnzloMEWTUT-",
        "outputId": "1d93cdb3-a434-41e2-d15c-b9c1f9ec02ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0 device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConvLSTMCell\n",
        "\n",
        "https://github.com/ndrplz/ConvLSTM_pytorch"
      ],
      "metadata": {
        "id": "AfJm0Ad0dApj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "\n",
        "        combined_conv = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters:\n",
        "        input_dim: Number of channels in input\n",
        "        hidden_dim: Number of hidden channels\n",
        "        kernel_size: Size of kernel in convolutions\n",
        "        num_layers: Number of LSTM layers stacked on each other\n",
        "        batch_first: Whether or not dimension 0 is the batch or not\n",
        "        bias: Bias or no bias in Convolution\n",
        "        return_all_layers: Return the list of computations for all layers\n",
        "        Note: Will do same padding.\n",
        "\n",
        "    Input:\n",
        "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
        "    Output:\n",
        "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
        "            0 - layer_output_list is the list of lists of length T of each output\n",
        "            1 - last_state_list is the list of last states\n",
        "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
        "    Example:\n",
        "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
        "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
        "        >> _, last_states = convlstm(x)\n",
        "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self._check_kernel_size_consistency(kernel_size)\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
        "\n",
        "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor: todo\n",
        "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
        "        hidden_state: todo\n",
        "            None. todo implement stateful\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        last_state_list, layer_output\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        b, _, _, h, w = input_tensor.size()\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            # Since the init is done in forward. Can send image size here\n",
        "            hidden_state = self._init_hidden(batch_size=b,\n",
        "                                             image_size=(h, w))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
        "                                                 cur_state=[h, c])\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h, c])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size, image_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "metadata": {
        "id": "1f9lmbq9dWgG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "iYTTICl6dx2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvLSTMNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, kernel_size, num_layers):\n",
        "        super(ConvLSTMNetwork, self).__init__()\n",
        "        self.convlstm = ConvLSTM(input_dim=input_dim,\n",
        "                                 hidden_dim=hidden_dims,\n",
        "                                 kernel_size=kernel_size,\n",
        "                                 num_layers=num_layers,\n",
        "                                 batch_first=True,\n",
        "                                 bias=True,\n",
        "                                 return_all_layers=False)\n",
        "        # The last hidden layer's output is used for final image prediction\n",
        "        final_hidden_dim = hidden_dims[-1]\n",
        "        self.final_conv = nn.Conv2d(final_hidden_dim, 1, kernel_size=1)  # Output 1 channel image\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should be of shape (batch, sequence, channels, height, width)\n",
        "        lstm_out, _ = self.convlstm(x)  # Get output from ConvLSTM\n",
        "        # lstm_out is a list where we need only the last output\n",
        "        last_output = lstm_out[-1][:, -1, :, :, :]  # Take the output of the last time step\n",
        "        final_output = self.final_conv(last_output)  # Convert to the required output image\n",
        "        return final_output\n",
        "\n",
        "# Parameters for Model Architecture:\n",
        "channels = 1  # Grayscale images\n",
        "hidden_dims = [64, 64, 128]  # Hidden dimensions for each ConvLSTM layer\n",
        "kernel_size = (3, 3)  # Kernel size for ConvLSTM cells\n",
        "num_layers = 3  # Number of layers in ConvLSTM\n",
        "\n",
        "model = ConvLSTMNetwork(input_dim=channels,\n",
        "                        hidden_dims=hidden_dims,\n",
        "                        kernel_size=kernel_size,\n",
        "                        num_layers=num_layers)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8--AYMYIccXO",
        "outputId": "259a022c-896f-4776-e462-8be05e297b52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvLSTMNetwork(\n",
              "  (convlstm): ConvLSTM(\n",
              "    (cell_list): ModuleList(\n",
              "      (0): ConvLSTMCell(\n",
              "        (conv): Conv2d(65, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (1): ConvLSTMCell(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (2): ConvLSTMCell(\n",
              "        (conv): Conv2d(192, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_conv): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input data: Batch size of 1, 5 time steps, 1 channel, 100x100 pixels\n",
        "input_tensor = torch.rand(1, 5, 1, 50, 50).to(device)\n",
        "# Forward pass through the model\n",
        "output_image = model(input_tensor)\n",
        "print(\"Output image shape:\", output_image.shape)  # Expected shape: (batch, channels, height, width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQUniJP8fJSz",
        "outputId": "13beb7b7-8298-430e-a1d4-c7fc8309a767"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output image shape: torch.Size([1, 1, 50, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_parameters(model):\n",
        "    total_params = 0\n",
        "    print(\"Layer-wise parameters:\\n\")\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        param = parameter.numel()\n",
        "        print(f\"{name}: {param}\")\n",
        "        total_params += param\n",
        "    print(f\"\\nTotal parameters: {total_params}\")\n",
        "\n",
        "print_model_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHv5ZzBJiw-B",
        "outputId": "04934641-fd75-4d81-f8ea-ea4f2c07aaf8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer-wise parameters:\n",
            "\n",
            "convlstm.cell_list.0.conv.weight: 149760\n",
            "convlstm.cell_list.0.conv.bias: 256\n",
            "convlstm.cell_list.1.conv.weight: 294912\n",
            "convlstm.cell_list.1.conv.bias: 256\n",
            "convlstm.cell_list.2.conv.weight: 884736\n",
            "convlstm.cell_list.2.conv.bias: 512\n",
            "final_conv.weight: 128\n",
            "final_conv.bias: 1\n",
            "\n",
            "Total parameters: 1330561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchsummary import summary\n",
        "#summary(model, input_size=(5, 1, 100, 100)) # the summary call breaks the model\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary # older depricated 'torchinfo' works\n",
        "\n",
        "# Assuming the model and input_tensor are defined as shown previously\n",
        "summary(model, input_sizes=(1, 5, 1, 400, 300))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g75NbrvejyHT",
        "outputId": "e2bacea7-841f-4c12-a008-37008849f842"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "ConvLSTMNetwork                          --\n",
              "├─ConvLSTM: 1-1                          --\n",
              "│    └─ModuleList: 2-1                   --\n",
              "│    │    └─ConvLSTMCell: 3-1            150,016\n",
              "│    │    └─ConvLSTMCell: 3-2            295,168\n",
              "│    │    └─ConvLSTMCell: 3-3            885,248\n",
              "├─Conv2d: 1-2                            129\n",
              "=================================================================\n",
              "Total params: 1,330,561\n",
              "Trainable params: 1,330,561\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "R3MtSMoVkO08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "1dEe0be_oQ-5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7qucxaMkQgX",
        "outputId": "eaecbdb6-dec7-46a6-ee80-690dcf89f72c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy zip file from Google Drive to local Colab env. and unzip\n",
        "!cp \"/content/drive/My Drive/game_frames.zip\" \"/content/game_frames.zip\"\n",
        "!unzip -q \"/content/game_frames.zip\" -d \"/content/game_frames\"\n",
        "!ls \"/content/game_frames\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Heev9REZxWCe",
        "outputId": "9d2bad46-b5b1-4137-e61c-df9cb4417206"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "game_frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, sequence_length=6):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            sequence_length (int): Number of images in each sequence.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.sequence_length = sequence_length\n",
        "        self.image_filenames = [f for f in sorted(os.listdir(root_dir)) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of possible sequences\n",
        "        return len(self.image_filenames) - (self.sequence_length - 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images = []\n",
        "        for i in range(self.sequence_length):\n",
        "            img_name = os.path.join(self.root_dir, self.image_filenames[idx + i])\n",
        "            image = Image.open(img_name).convert('L')  # Convert to grayscale\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            images.append(image)\n",
        "\n",
        "        # Stack images to create a sequence tensor\n",
        "        # Assumes that images are transformed to tensors by `transforms`\n",
        "        sequence = torch.stack(images[:-1])  # All but last for input sequence\n",
        "        target = images[-1]  # Last image as ground truth\n",
        "        return sequence, target\n",
        "\n",
        "# Transform to tensor and resize if necessary\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),  # Resize all images to the same size\n",
        "    transforms.ToTensor(),  # Convert images to tensor\n",
        "    # If BCEWithLogitsLoss do not use normalization\n",
        "    #transforms.Normalize((0.5,), (0.5,))  # Normalize images; mean and std are tuples with one value per channel\n",
        "])"
      ],
      "metadata": {
        "id": "vA3EKnOboY9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageSequenceDataset('/content/game_frames/game_frames', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)  # Set `shuffle=False` to maintain sequence order !!!"
      ],
      "metadata": {
        "id": "wGsI4T19o9iC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "for sequences, targets in dataloader:\n",
        "    print(\"Batch of sequences shape:\", sequences.shape)\n",
        "    print(\"Batch of sequences type:\", sequences.dtype)\n",
        "    print(\"Batch of targets shape:\", targets.shape)\n",
        "    print(\"Batch of targets type:\", targets.dtype)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb_m4fkXNv8O",
        "outputId": "f380995b-b136-46fc-e440-fd7712e753d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of sequences shape: torch.Size([16, 5, 1, 100, 100])\n",
            "Batch of sequences type: torch.float32\n",
            "Batch of targets shape: torch.Size([16, 1, 100, 100])\n",
            "Batch of targets type: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "piSQ1DankRY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Loss function\n",
        "#criterion = torch.nn.MSELoss() # mean square error\n",
        "#criterion = torch.nn.L1Loss() # average of absolute differences between targets and predictions\n",
        "#criterion = torch.nn.HuberLoss() # quadratic for small errors and linear for large errors\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# https://discuss.pytorch.org/t/how-to-calculate-loss-for-binary-images/158987/6"
      ],
      "metadata": {
        "id": "bUVGoIzNkSrb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Y3dAOwt7upzs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "\n",
        "        for sequences, targets in progress_bar:\n",
        "            sequences = sequences.to(device)  # Shape: [batch_size, seq_length-1, channels, height, width]\n",
        "            targets = targets.to(device)  # Shape: [batch_size, channels, height, width]\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Update the progress bar description with the latest loss\n",
        "            progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss / (progress_bar.n + 1)}\")\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {epoch_loss}\")\n",
        "\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "ng3DQ-DguyN4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, criterion, optimizer, num_epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8e232e16c1c843e5a3059de970e43b33",
            "c34ba1b2ace54577b319f95a820fca5a",
            "615b2cd3ab05426b9dfae2d74878a36e",
            "7c5a0ce99efd49548be601ba64b02636",
            "bfdf007f33074cd080ac36bb2a41d53c",
            "1d4ef4a2abac4228b7d26cfd60a5925c",
            "fa6357cd29644247b46ad03fe24c0679",
            "3593345bd79641559bf2e00e954409a4",
            "8ace97cf3152497aafd37e7819b73a99",
            "04c2d3846a804fd7b1f2496a2db10f7c",
            "4e952cc773574dc88e561ce8337a63f6"
          ]
        },
        "id": "ofohIyefwsSN",
        "outputId": "64613cd4-ccc3-4549-866a-2eba13aaead4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/1:   0%|          | 0/625 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e232e16c1c843e5a3059de970e43b33"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Model"
      ],
      "metadata": {
        "id": "ppUM-o-ou_lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model, 'dx_ball.pt')\n",
        "# Save only the state dictionary (model weights)\n",
        "torch.save(model.state_dict(), 'dx_ball_weights.pt')"
      ],
      "metadata": {
        "id": "F2MLYr5Zu-5S"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model, '/content/drive/My Drive/dx_ball.pt')\n",
        "# Save only the state dictionary (model weights)\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/dx_ball_weights.pt')"
      ],
      "metadata": {
        "id": "oHWjY9DhwzDB"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}
