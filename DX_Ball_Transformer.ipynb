{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a66c2222-974e-423e-a40b-569ee52b9b3b",
      "metadata": {
        "id": "a66c2222-974e-423e-a40b-569ee52b9b3b"
      },
      "source": [
        "# Transformer\n",
        "\n",
        "<img src='https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png' width=800/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3666a9-e52b-46f9-bbba-36bebe8bb291",
      "metadata": {
        "id": "5e3666a9-e52b-46f9-bbba-36bebe8bb291"
      },
      "source": [
        "## Content\n",
        "- [Self- & Multi-Head-Attention](#Self--&-Multi-Head-Attention)\n",
        "- [Transformer Encoder](#Transformer-Encoder)\n",
        "- [Input Embedding](#Input-Embedding)\n",
        "- [Train a Vision Transformer](#Train-a-Vision-Transformer)\n",
        "  - Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5032e6b1-b0a7-4493-87e6-5014548896f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5032e6b1-b0a7-4493-87e6-5014548896f2",
        "outputId": "f7dfc415-2920-4393-99e7-56674612fe14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0 device\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim, Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from einops import einsum, rearrange\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "139b3517-6604-44df-ab1e-0f3a2088ec4f",
      "metadata": {
        "id": "139b3517-6604-44df-ab1e-0f3a2088ec4f"
      },
      "source": [
        "## Self- & Multi-Head Attention\n",
        "\n",
        "Aus dem Paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) (2017)\n",
        "\n",
        "$$ Attention(Q, K, V) = softmax \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d7467fd1-5bc2-4743-bacc-7902e33e8249",
      "metadata": {
        "id": "d7467fd1-5bc2-4743-bacc-7902e33e8249"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, dim: int, head_size: int):\n",
        "        \"\"\"\n",
        "        One Head of Self Attention containing 3 linear layers to\n",
        "        project an input into query, key and value, and perform\n",
        "        the self attention mechanism.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(dim, head_size, bias=False) # query\n",
        "        self.k = nn.Linear(dim, head_size, bias=False) # key\n",
        "        self.v = nn.Linear(dim, head_size, bias=False) # value\n",
        "\n",
        "        # if query and key are unit variance,\n",
        "        # the scaled dot product will be unit variance too\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x: Tensor of shape [B, N, C]\n",
        "\n",
        "        Returns: Tensor of shape [B, head_size, C]\n",
        "        \"\"\"\n",
        "        q = self.q(x)  # [B, N, C]\n",
        "        k = self.k(x)  # [B, N, C]\n",
        "        v = self.v(x)  # [B, N, C]\n",
        "\n",
        "        scores = einsum(q, k, 'B N C, B M C -> B N M') * self.scale  # [B, N, N]\n",
        "        weights = scores.softmax(dim=-1)\n",
        "        context = einsum(weights, v, 'B N M, B M C -> B N C')\n",
        "        return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f9e64f26-fae8-4be9-9912-41fd76900b23",
      "metadata": {
        "id": "f9e64f26-fae8-4be9-9912-41fd76900b23"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi Head Attention Module which applies 'heads' times SelfAttention\n",
        "    on the input.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, heads: int, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0, \"dim must be a multiple of heads\"\n",
        "        headsize = dim // heads\n",
        "        self.heads = nn.ModuleList([Head(dim, headsize) for _ in range(heads)])\n",
        "        self.proj  = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)  # Regularization\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x: Tensor of shape [B, N, C]\n",
        "\n",
        "        Returns: Tensor of shape [B, N, C]\n",
        "        \"\"\"\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=2)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)  # Regularization\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f7ad8b-be47-4c88-8214-287973ea47a4",
      "metadata": {
        "id": "78f7ad8b-be47-4c88-8214-287973ea47a4"
      },
      "source": [
        "## Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "de49d20d-59b4-4787-bad5-18da90900671",
      "metadata": {
        "id": "de49d20d-59b4-4787-bad5-18da90900671"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim: int, heads: int, ff_dim: int = None, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(dim, heads, dropout)\n",
        "        self.ffwd = FeedForward(dim, ff_dim, dropout)\n",
        "        self.ln1 = nn.LayerNorm(dim)\n",
        "        self.ln2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "614b5768-c9d0-4821-a2cf-403a310ce6e4",
      "metadata": {
        "id": "614b5768-c9d0-4821-a2cf-403a310ce6e4"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim: int, ff_dim: int = None, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        ff_dim = ff_dim or dim * 4  # Default to 4x hidden dimension\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, ff_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(ff_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e8b72442-37d2-4749-9437-0c5261992a96",
      "metadata": {
        "id": "e8b72442-37d2-4749-9437-0c5261992a96"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim: int, depth: int, heads: int, ff_dim: int = None, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([Block(dim, heads, ff_dim, dropout) for _ in range(depth)])\n",
        "        self.ln = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.ln(x)  # Final layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a17a485f-5614-4f00-b91e-95ae99903c87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a17a485f-5614-4f00-b91e-95ae99903c87",
        "outputId": "4c9de08b-297a-4b03-9c74-62ef938c6f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 32, 16])\n",
            "Output shape: torch.Size([4, 32, 16])\n"
          ]
        }
      ],
      "source": [
        "# Define dimensions\n",
        "B, N, C = 4, 32, 16  # Batch size, Number of tokens, Embedding dimension\n",
        "\n",
        "# Create random input tensor\n",
        "batch = torch.randn(B, N, C)  # Shape: [Batch size, Sequence length, Embedding dimension]\n",
        "\n",
        "# Define TransformerEncoder parameters\n",
        "dim = C  # Embedding dimension\n",
        "depth = 4  # Number of Transformer blocks\n",
        "heads = 4  # Number of attention heads\n",
        "ff_dim = 64  # Feed-forward network dimension\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Instantiate TransformerEncoder\n",
        "encoder = TransformerEncoder(dim=dim, depth=depth, heads=heads, ff_dim=ff_dim, dropout=dropout)\n",
        "\n",
        "# Pass the batch through the TransformerEncoder\n",
        "output = encoder(batch)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Input shape:\", batch.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7c8a92-07b6-4db4-b50b-19a29619847f",
      "metadata": {
        "id": "3c7c8a92-07b6-4db4-b50b-19a29619847f"
      },
      "source": [
        "# Input Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96dbbcc8-7da6-4a74-b622-ec3f3ba5302f",
      "metadata": {
        "id": "96dbbcc8-7da6-4a74-b622-ec3f3ba5302f"
      },
      "source": [
        "### Text Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5f8d3386-68dc-44e5-b81a-5e68192402ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f8d3386-68dc-44e5-b81a-5e68192402ad",
        "outputId": "341a6466-ded0-47e9-e305-6ffc3130eef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            " DHLWabcdefgilnoprst\n"
          ]
        }
      ],
      "source": [
        "# Example for Text tokens:\n",
        "\n",
        "text = \"Hallo Welt des Deep Learnings abcdefg\"\n",
        "\n",
        "chars = sorted(set(text))\n",
        "print(len(chars))\n",
        "print(\"\".join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c8b564b1-58f1-4f9b-b18f-eccba59a852c",
      "metadata": {
        "id": "c8b564b1-58f1-4f9b-b18f-eccba59a852c"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "\n",
        "stoi = { ch:i for i, ch in enumerate(chars)}\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join(itos[i] for i in l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9f584547-fa6f-489b-bf3b-b5b8dfa0af25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f584547-fa6f-489b-bf3b-b5b8dfa0af25",
        "outputId": "f1cfc555-7d9a-41d7-b975-23fc90fccd21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  6,  7,  8,  9],\n",
              "        [ 7,  8,  9, 10, 11]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "batch = torch.tensor([\n",
        "    encode(\"abcde\"),\n",
        "    encode(\"cdefg\")\n",
        "])\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1a028a36-a305-4edf-88e6-9a2b83092398",
      "metadata": {
        "id": "1a028a36-a305-4edf-88e6-9a2b83092398"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(20, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b81d7ad0-aee4-4a39-ac1b-011dc330d566",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b81d7ad0-aee4-4a39-ac1b-011dc330d566",
        "outputId": "0327520e-c5a3-4e55-ebc7-c7ecb78c325d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "emb = embedding(batch)\n",
        "emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "724f2684-ac19-48c7-ac17-823bec92adb0",
      "metadata": {
        "id": "724f2684-ac19-48c7-ac17-823bec92adb0"
      },
      "source": [
        "# Vision Transformer (ViT) for Image Inputs\n",
        "[An Image Is Worth 16X16 Words](https://arxiv.org/pdf/2010.11929.pdf)\n",
        "\n",
        "<img src='https://production-media.paperswithcode.com/social-images/UhPqfdxgjZGSAsbC.png' width=1200/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame Predictor Transformer"
      ],
      "metadata": {
        "id": "D_sHJ8FbSlTQ"
      },
      "id": "D_sHJ8FbSlTQ"
    },
    {
      "cell_type": "code",
      "source": [
        "class FramePredictor(nn.Module):\n",
        "    def __init__(self, seq_size=5, img_size=50, patch_size=10, dim=128, depth=4, heads=4):\n",
        "        super().__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.dim = dim\n",
        "\n",
        "        # Patch embedding\n",
        "        self.embedding = nn.Conv2d(seq_size, dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder = TransformerEncoder(dim, depth, heads)\n",
        "\n",
        "        # Output projection\n",
        "        self.to_image = nn.ConvTranspose2d(dim, 1, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [batch_size, sequence_length, channels, height, width]\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, channels, height, width = x.shape\n",
        "\n",
        "        # Combine sequence_length into the channel dimension\n",
        "        x = x.view(batch_size, seq_length * channels, height, width)  # [batch_size, sequence_length * channels, height, width]\n",
        "\n",
        "        # Apply Conv2d patch embedding\n",
        "        x = self.embedding(x)  # [batch_size, dim, num_patches_y, num_patches_x]\n",
        "\n",
        "        # Flatten patches and prepare for Transformer\n",
        "        num_patches = x.size(2) * x.size(3)  # Total number of patches\n",
        "        x = x.flatten(2).transpose(1, 2)  # [batch_size, num_patches, dim]\n",
        "\n",
        "        # Pass through Transformer encoder\n",
        "        x = self.encoder(x)  # [batch_size, num_patches, dim]\n",
        "\n",
        "        # Reshape and reconstruct patches\n",
        "        x = x.transpose(1, 2).view(batch_size, self.dim, height // self.patch_size, width // self.patch_size)\n",
        "        return self.to_image(x)  # [batch_size, 1, height, width]"
      ],
      "metadata": {
        "id": "kF4EXcJASqSb"
      },
      "id": "kF4EXcJASqSb",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = FramePredictor(seq_size=10, img_size=50, patch_size=10, dim=128, depth=4, heads=4)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ0E6aZsqVww",
        "outputId": "44bef360-34e2-41f6-ac22-df2ab6adacba"
      },
      "id": "DQ0E6aZsqVww",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FramePredictor(\n",
              "  (embedding): Conv2d(10, 128, kernel_size=(10, 10), stride=(10, 10))\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x Block(\n",
              "        (attn): MultiHeadAttention(\n",
              "          (heads): ModuleList(\n",
              "            (0-3): 4 x Head(\n",
              "              (q): Linear(in_features=128, out_features=32, bias=False)\n",
              "              (k): Linear(in_features=128, out_features=32, bias=False)\n",
              "              (v): Linear(in_features=128, out_features=32, bias=False)\n",
              "            )\n",
              "          )\n",
              "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ffwd): FeedForward(\n",
              "          (net): Sequential(\n",
              "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "            (3): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (to_image): ConvTranspose2d(128, 1, kernel_size=(10, 10), stride=(10, 10))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary # older depricated 'torchinfo' works\n",
        "\n",
        "# Assuming the model and input_tensor are defined as shown previously\n",
        "summary(model, input_sizes=(1, 10, 1, 50, 50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_O1gv7-qdnS",
        "outputId": "dfce4f6d-117b-4bd7-ff2c-bbb847ba8640"
      },
      "id": "Y_O1gv7-qdnS",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===========================================================================\n",
              "Layer (type:depth-idx)                             Param #\n",
              "===========================================================================\n",
              "FramePredictor                                     --\n",
              "├─Conv2d: 1-1                                      128,128\n",
              "├─TransformerEncoder: 1-2                          --\n",
              "│    └─ModuleList: 2-1                             --\n",
              "│    │    └─Block: 3-1                             197,888\n",
              "│    │    └─Block: 3-2                             197,888\n",
              "│    │    └─Block: 3-3                             197,888\n",
              "│    │    └─Block: 3-4                             197,888\n",
              "│    └─LayerNorm: 2-2                              256\n",
              "├─ConvTranspose2d: 1-3                             12,801\n",
              "===========================================================================\n",
              "Total params: 932,737\n",
              "Trainable params: 932,737\n",
              "Non-trainable params: 0\n",
              "==========================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input: [batch_size=24, sequence_length=5, channels=1, height=50, width=50]\n",
        "dummy_input = torch.randn(24, 10, 1, 50, 50).to(device)\n",
        "\n",
        "# Forward pass\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)  # Expected: [24, 1, 50, 50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtHrRvHyPHt6",
        "outputId": "b8ced7cb-5e7c-41e7-d277-c77bae65dc37"
      },
      "id": "FtHrRvHyPHt6",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([24, 1, 50, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "psnGzCwcm_Ob"
      },
      "id": "psnGzCwcm_Ob"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "GPXozmcGicbZ"
      },
      "id": "GPXozmcGicbZ",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "50edffb8-29d9-4f41-9565-4cff2686bab8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50edffb8-29d9-4f41-9565-4cff2686bab8",
        "outputId": "a30117cb-b1dd-427b-b4fd-7a7dbbd31cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy zip file from Google Drive to local Colab env. and unzip\n",
        "!cp \"/content/drive/My Drive/game_frames.zip\" \"/content/game_frames.zip\"\n",
        "!unzip -q \"/content/game_frames.zip\" -d \"/content/game_frames\"\n",
        "!ls \"/content/game_frames\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OalWkZLsnJIC",
        "outputId": "1e5979af-762b-49dc-d70f-a9568c1cbe75"
      },
      "id": "OalWkZLsnJIC",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/game_frames/game_frames/000001.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/game_frames/game_frames/000002.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: nA\n",
            "replace /content/game_frames/game_frames/000003.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "game_frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, sequence_length=11):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            sequence_length (int): Number of images in each sequence.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.sequence_length = sequence_length\n",
        "        self.image_filenames = [f for f in sorted(os.listdir(root_dir)) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of possible sequences\n",
        "        return len(self.image_filenames) - (self.sequence_length - 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images = []\n",
        "        for i in range(self.sequence_length):\n",
        "            img_name = os.path.join(self.root_dir, self.image_filenames[idx + i])\n",
        "            image = Image.open(img_name).convert('L')  # Convert to grayscale\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            images.append(image)\n",
        "\n",
        "        # Stack images to create a sequence tensor\n",
        "        # Assumes that images are transformed to tensors by `transforms`\n",
        "        sequence = torch.stack(images[:-1])  # All but last for input sequence\n",
        "        target = images[-1]  # Last image as ground truth\n",
        "        return sequence, target\n",
        "\n",
        "# Transform to tensor and resize if necessary\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((50, 50)),  # Resize all images to the same size\n",
        "    transforms.ToTensor(),  # Convert images to tensor\n",
        "    # If BCEWithLogitsLoss do not use normalization\n",
        "    #transforms.Normalize((0.5,), (0.5,))  # Normalize images; mean and std are tuples with one value per channel\n",
        "])"
      ],
      "metadata": {
        "id": "-bfcqKsVnORb"
      },
      "id": "-bfcqKsVnORb",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageSequenceDataset('/content/game_frames/game_frames', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=24, shuffle=False)  # Set `shuffle=False` to maintain sequence order !!!"
      ],
      "metadata": {
        "id": "AKFaUr2WneZB"
      },
      "id": "AKFaUr2WneZB",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "for sequences, targets in dataloader:\n",
        "    print(\"Batch of sequences shape:\", sequences.shape)\n",
        "    print(\"Batch of sequences type:\", sequences.dtype)\n",
        "    print(\"Batch of targets shape:\", targets.shape)\n",
        "    print(\"Batch of targets type:\", targets.dtype)\n",
        "    print(\"\")\n",
        "    print(\"Sequence min value:\", sequences.min().item())\n",
        "    print(\"Sequence max value:\", sequences.max().item())\n",
        "    print(\"Target min value:\", targets.min().item())\n",
        "    print(\"Target max value:\", targets.max().item())\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkq1WAcFnhip",
        "outputId": "2d210847-4083-446f-b7fa-ca2e097b0065"
      },
      "id": "Hkq1WAcFnhip",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of sequences shape: torch.Size([24, 10, 1, 50, 50])\n",
            "Batch of sequences type: torch.float32\n",
            "Batch of targets shape: torch.Size([24, 1, 50, 50])\n",
            "Batch of targets type: torch.float32\n",
            "\n",
            "Sequence min value: 0.0\n",
            "Sequence max value: 1.0\n",
            "Target min value: 0.0\n",
            "Target max value: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "jmMx1851pCkj"
      },
      "id": "jmMx1851pCkj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary Cross Entropy (BCE)**\n",
        "\n",
        "$Loss=−[y⋅log(p)+(1−y)⋅log(1−p)]$\n",
        "\n",
        "  - $y$ ground truth\n",
        "  - $p$ predicted\n",
        "\n",
        "$Loss=−[pos\\_weight⋅y⋅log(p)+(1−y)⋅log(1−p)]$\n",
        "\n",
        "  - positive weight multiplyer"
      ],
      "metadata": {
        "id": "UgR0-Cl-bzqV"
      },
      "id": "UgR0-Cl-bzqV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "#criterion = nn.BCEWithLogitsLoss()  # For binary prediction\n",
        "#criterion = nn.MSELoss(reduction='sum')\n",
        "#criterion = nn.L1Loss(reduction='sum')\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='sum', pos_weight=torch.tensor([4.0]).to(device))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n"
      ],
      "metadata": {
        "id": "MEihR2LYn4xP"
      },
      "id": "MEihR2LYn4xP",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Train a PyTorch model using the provided DataLoader, criterion, and optimizer.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to be trained.\n",
        "        dataloader (DataLoader): The DataLoader providing training data.\n",
        "        criterion (nn.Module): The loss function.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer for training.\n",
        "        num_epochs (int, optional): Number of epochs to train. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "\n",
        "        for sequences, targets in progress_bar:\n",
        "            sequences = sequences.to(device)  # Shape: [batch_size, seq_length-1, channels, height, width]\n",
        "            targets = targets.to(device)  # Shape: [batch_size, channels, height, width]\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, targets)/2500 # manual normalizing\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Update the progress bar description with the latest loss\n",
        "            progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss / (progress_bar.n + 1)}\")\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {epoch_loss}\")\n",
        "\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "0iP9nMqIq6ff"
      },
      "id": "0iP9nMqIq6ff",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ211tH1sGqO",
        "outputId": "902d45e5-0ca8-4f28-eb5a-4a284f54b7b9"
      },
      "id": "FJ211tH1sGqO",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.3328281421005726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Loss: 0.313997524279356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Loss: 0.28100937019586564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Loss: 0.25387811896800994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Loss: 0.253305420178175\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model, '/content/drive/My Drive/dx_ball_transformer_model.pt')\n",
        "# Save only the state dictionary (model weights)\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/dx_ball_transformer_weights.pt')"
      ],
      "metadata": {
        "id": "EykfAUPvnzMm"
      },
      "id": "EykfAUPvnzMm",
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}